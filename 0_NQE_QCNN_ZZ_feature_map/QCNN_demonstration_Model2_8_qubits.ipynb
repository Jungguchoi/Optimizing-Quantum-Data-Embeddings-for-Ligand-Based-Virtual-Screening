{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noiseless QCNN (8-qubits) demo for Model 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo uses 8-qubit Quantum Convolutional Neural Network (QCNN) to see how pre-training the quantum embedding can be helpful for training a parameterized QML circuits for classfication tasks.\n",
    "\n",
    "If you are interested in the details about the QCNN used in this demo, check out https://arxiv.org/pdf/2108.00661.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pennylane import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import sys\n",
    "\n",
    "import Hybrid_nn\n",
    "import torch\n",
    "from torch import nn\n",
    "import data\n",
    "import pennylane as qml\n",
    "import embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset with four features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device('default.qubit', wires=8)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "feature_reduction = False\n",
    "classes = [0,1]\n",
    "X_train, X_test, Y_train, Y_test = data.data_load_and_process('protein', feature_reduction, classes)\n",
    "\n",
    "def new_data(batch_size, X, Y):\n",
    "    X1_new, X2_new, Y_new = [], [], []\n",
    "    for i in range(batch_size):\n",
    "        n, m = np.random.randint(len(X)), np.random.randint(len(X))\n",
    "        X1_new.append(X[n])\n",
    "        X2_new.append(X[m])\n",
    "        if Y[n] == Y[m]:\n",
    "            Y_new.append(1)\n",
    "        else:\n",
    "            Y_new.append(0)\n",
    "    return torch.tensor(X1_new).to(device), torch.tensor(X2_new).to(device), torch.tensor(Y_new).to(device)\n",
    "\n",
    "N_valid, N_test = 68, 68\n",
    "X1_new_valid, X2_new_valid, Y_new_valid = new_data(N_valid, X_test, Y_test)\n",
    "X1_new_test, X2_new_test, Y_new_test = new_data(N_test, X_test, Y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1: Pre-Training the Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Circuit for evaluating Model2_Fidelity and Model2_HSinner for 8 qubits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def Four_circuit2(inputs): \n",
    "    embedding.Four_QuantumEmbedding2(inputs[0:16])\n",
    "    embedding.Four_QuantumEmbedding2_inverse(inputs[16:32])\n",
    "    return qml.probs(wires=range(8))\n",
    "\n",
    "# 4 qubit Model2_Fidelity\n",
    "class Four_Model2_Fidelity(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.qlayer2 = qml.qnn.TorchLayer(Four_circuit2, weight_shapes={})\n",
    "        self.linear_relu_stack2 = nn.Sequential(\n",
    "            nn.Linear(1972, 256)\n",
    "        )\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.linear_relu_stack2(x1)\n",
    "        #x2 = self.linear_relu_stack2(x2)\n",
    "        #x = torch.concat([x1, x2], 1)\n",
    "        #x = self.qlayer2(x)\n",
    "        return x1[:,0]\n",
    "\n",
    "# 4 qubit Model2_HSinner\n",
    "class Four_Model2_HSinner(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.matrix_fn2 = qml.matrix(Four_circuit2)\n",
    "        self.linear_relu_stack2 = nn.Sequential(\n",
    "            nn.Linear(4, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 8)\n",
    "        )\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.linear_relu_stack2(x1)\n",
    "        x2 = self.linear_relu_stack2(x2)\n",
    "        x = torch.concat([x1, x2], 1).to(\"cpu\")\n",
    "        x = [torch.real(torch.trace(self.matrix_fn2(a))) for a in x]\n",
    "        x = torch.stack(x, dim=0).to(device)\n",
    "        return x / 2**4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the distances of Test dataset with the pre-trained quantum embeddings. From the calculated trace distance gain the lower bound of the linear loss function (with respect to the test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_test, X0_test = [], []\n",
    "for i in range(len(X_test)):\n",
    "    if Y_test[i] == 1:\n",
    "        X1_test.append(X_test[i])\n",
    "    else:\n",
    "        X0_test.append(X_test[i])\n",
    "X1_test, X0_test = torch.tensor(X1_test), torch.tensor(X0_test)\n",
    "\n",
    "X1_train, X0_train = [], []\n",
    "for i in range(len(X_train)):\n",
    "    if Y_train[i] == 1:\n",
    "        X1_train.append(X_train[i])\n",
    "    else:\n",
    "        X0_train.append(X_train[i])\n",
    "X1_train, X0_train = torch.tensor(X1_train), torch.tensor(X0_train)\n",
    "\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def Four_Distance2(inputs): \n",
    "    embedding.Four_QuantumEmbedding2(inputs[0:16])\n",
    "    return qml.density_matrix(wires=range(8))\n",
    "\n",
    "class Distances2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.qlayer2_distance = qml.qnn.TorchLayer(Four_Distance2, weight_shapes={})\n",
    "        self.linear_relu_stack2 = nn.Sequential(\n",
    "            nn.Linear(1972, 256)\n",
    "        )\n",
    "    def forward(self, x1, x0, Distance, Trained):\n",
    "        if Trained:\n",
    "            x1 = self.linear_relu_stack2(x1.float())\n",
    "            x0 = self.linear_relu_stack2(x0.float())\n",
    "        rhos1 = self.qlayer2_distance(x1)\n",
    "        rhos0 = self.qlayer2_distance(x0)\n",
    "        rho1 = torch.sum(rhos1, dim=0) / len(x1)\n",
    "        rho0 = torch.sum(rhos0, dim=0) / len(x0)\n",
    "        rho_diff = rho1 - rho0\n",
    "        if Distance == 'Trace':\n",
    "            eigvals = torch.linalg.eigvals(rho_diff)\n",
    "            return 0.5 * torch.real(torch.sum(torch.abs(eigvals)))\n",
    "        elif Distance == 'Hilbert-Schmidt':\n",
    "            return 0.5 * torch.trace(rho_diff @ rho_diff)\n",
    "\n",
    "\n",
    "PATH_Model2_Fidelity = '/Users/jungguchoi/Library/Mobile Documents/com~apple~CloudDocs/1_Post_doc(Cleveland_clinic:2024.10~2025.09)/1_Research_project/3_quantum_embedding_comparison_sequence(2024.09 ~ XXXX.XX)/2_exp/9_NQE_QCNN_MNIST_8_qubits/Neural-Quantum-Embedding-main-8-qubits/Results/Model_Amplitude_Protein_8_qubits2.pt'\n",
    "#PATH_Model2_HSinner = '/Users/tak/Github/QEmbedding/Results/QCNN_demonstration/Noiseless/Model 2/Four_Model2_HSinner.pt'\n",
    "Model2_Fidelity_Distance = Distances2().to(device)\n",
    "Model2_Fidelity_Distance.load_state_dict(torch.load(PATH_Model2_Fidelity, map_location=device), strict=False)\n",
    "#Model2_HSinner_Distance = Distances2().to(device)\n",
    "#Model2_HSinner_Distance.load_state_dict(torch.load(PATH_Model2_HSinner, map_location=device))\n",
    "\n",
    "# Calculated from Model1 \n",
    "Trace_before_traindata = 0.09861467783124321\n",
    "Trace_before_testdata = 0.16495842609348835\n",
    "LB_before_traindata = 0.5 * (1 - Trace_before_traindata)\n",
    "\n",
    "print(f\"Trace Distance (Training Data) Before: {Trace_before_traindata}\")\n",
    "print(f\"Trace Distance (Test Data) Before: {Trace_before_testdata}\")\n",
    "\n",
    "# Distances After training with Model2_Fidelity\n",
    "Trace_Fidelity_traindata = Model2_Fidelity_Distance(X1_train, X0_train, 'Trace', True)\n",
    "Trace_Fidelity_testdata = Model2_Fidelity_Distance(X1_test, X0_test, 'Trace', True)\n",
    "print(f\"Trace Distance (Training Data) After Model2 Fidelity: {Trace_Fidelity_traindata}\")\n",
    "print(f\"Trace Distance (Test Data) After Model2 Fidelity: {Trace_Fidelity_testdata}\")\n",
    "\n",
    "# Distances After training with Model2_HSinner\n",
    "#Trace_HSinner_traindata = Model2_HSinner_Distance(X1_train, X0_train, 'Trace', True)\n",
    "#Trace_HSinner_testdata = Model2_HSinner_Distance(X1_test, X0_test, 'Trace', True)\n",
    "#print(f\"Trace Distance (Training Data) After Model2 HSinner: {Trace_HSinner_traindata}\")\n",
    "#print(f\"Trace Distance (Test Data) After Model2 HSinner: {Trace_HSinner_testdata}\")\n",
    "\n",
    "# Lower Bounds\n",
    "LB_before_traindata = 0.5 * (1 - Trace_before_traindata)\n",
    "LB_Fidelity_traindata = 0.5 * (1 - Trace_Fidelity_traindata.detach().numpy())\n",
    "#LB_HSinner_traindata = 0.5 * (1 - Trace_HSinner_traindata.detach().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2: Training QCNN with/without Pre-trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = [-1 if y == 0 else 1 for y in Y_train]\n",
    "Y_test = [-1 if y == 0 else 1 for y in Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class x_transform2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack2 = nn.Sequential(\n",
    "            torch.nn.Linear(177, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 16)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack2(x)\n",
    "        return x.detach().numpy()\n",
    "\n",
    "model = x_transform2().to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tunable Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 2000\n",
    "learning_rate = 0.008\n",
    "batch_size = 25\n",
    "ansatz = 'SU4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statepreparation(x, Trained):\n",
    "    if Trained == 'Model2_Fidelity':\n",
    "        model.load_state_dict(torch.load(PATH_Model2_Fidelity, map_location=device))\n",
    "        x = model(torch.tensor(x).float())\n",
    "    elif Trained == 'Model2_HSinner':\n",
    "        model.load_state_dict(torch.load(PATH_Model2_HSinner, map_location=device))\n",
    "        x = model(torch.tensor(x))\n",
    "    embedding.Four_QuantumEmbedding2(x)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def QCNN_classifier(params, x, Trained):\n",
    "    statepreparation(x, Trained)\n",
    "    embedding.QCNN_eight(params, ansatz)\n",
    "    return qml.expval(qml.PauliZ(2))\n",
    "\n",
    "\n",
    "def Linear_Loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l,p in zip(labels, predictions):\n",
    "        loss += 0.5 * (1 - l * p)\n",
    "    return loss / len(labels)\n",
    "\n",
    "def cost(weights, X_batch, Y_batch, Trained):\n",
    "    preds = [QCNN_classifier(weights, x, Trained) for x in X_batch]\n",
    "    return Linear_Loss(Y_batch, preds)\n",
    "\n",
    "\n",
    "def circuit_training(X_train, Y_train, Trained):\n",
    "\n",
    "    if ansatz == 'SU4':\n",
    "        num_weights = 45\n",
    "    elif ansatz == 'U_6':\n",
    "        num_weights = 20\n",
    "    elif ansatz == 'TTN':\n",
    "        num_weights = 4\n",
    "\n",
    "    weights = np.random.random(num_weights, requires_grad = True)\n",
    "    opt = qml.NesterovMomentumOptimizer(stepsize=learning_rate)\n",
    "    loss_history = []\n",
    "    for it in range(steps):\n",
    "        batch_index = np.random.randint(0, len(X_train), (batch_size,))\n",
    "        X_batch = [X_train[i] for i in batch_index]\n",
    "        Y_batch = [Y_train[i] for i in batch_index]\n",
    "        weights, cost_new = opt.step_and_cost(lambda v: cost(v, X_batch, Y_batch, Trained),\n",
    "                                                     weights)\n",
    "        loss_history.append(cost_new)\n",
    "        if it % 10 == 0:\n",
    "            print(\"iteration: \", it, \" cost: \", cost_new)\n",
    "    return loss_history, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Loss_histories_not_trained, weights_not_trained = [], []\n",
    "Loss_histories_Model2_Fidelity, weights_Model2_Fidelity = [], []\n",
    "for i in range(5):\n",
    "    loss_not_trained, weight_not_trained = circuit_training(X_train, Y_train, False)\n",
    "    loss_Model2_Fidelity, weight_Model2_Fidelity = circuit_training(X_train, Y_train, 'Model2_Fidelity')\n",
    "    #loss_Model2_HSinner, weight_Model2_HSinner = circuit_training(X_train, Y_train, 'Model2_HSinner')\n",
    "\n",
    "    Loss_histories_not_trained.append(loss_not_trained)\n",
    "    weights_not_trained.append(weight_not_trained)\n",
    "\n",
    "    Loss_histories_Model2_Fidelity.append(loss_Model2_Fidelity)\n",
    "    weights_Model2_Fidelity.append(weight_Model2_Fidelity)\n",
    "\n",
    "    #Loss_histories_Model2_HSinner.append(loss_Model2_HSinner)\n",
    "    #weights_Model2_HSinner.append(weight_Model2_HSinner)\n",
    "\n",
    "Loss_histories_not_trained = np.array(Loss_histories_not_trained)\n",
    "Loss_histories_Model2_Fidelity = np.array(Loss_histories_Model2_Fidelity) \n",
    "#Loss_histories_Model2_HSinner = np.array(Loss_histories_Model2_HSinner)\n",
    "\n",
    "Not_trained_mean ,Not_trained_std = Loss_histories_not_trained.mean(axis=0), Loss_histories_not_trained.std(axis=0)\n",
    "Model2_Fidelity_mean, Model2_Fidelity_std = Loss_histories_Model2_Fidelity.mean(axis=0), Loss_histories_Model2_Fidelity.std(axis=0)\n",
    "#Model2_HSinner_mean, Model2_HSinner_std = Loss_histories_Model2_HSinner.mean(axis=0), Loss_histories_Model2_HSinner.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Loss_histories_not_trained), len(weights_not_trained))\n",
    "print(len(Loss_histories_Model2_Fidelity), len(weights_Model2_Fidelity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss_histories_not_trained = np.array(Loss_histories_not_trained)\n",
    "Loss_histories_Model2_Fidelity = np.array(Loss_histories_Model2_Fidelity) \n",
    "\n",
    "Not_trained_mean ,Not_trained_std = Loss_histories_not_trained.mean(axis=0), Loss_histories_not_trained.std(axis=0)\n",
    "Model2_Fidelity_mean, Model2_Fidelity_std = Loss_histories_Model2_Fidelity.mean(axis=0), Loss_histories_Model2_Fidelity.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "fig, ax = plt.subplots()\n",
    "clrs = sns.color_palette(\"husl\", 3)\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    ax.plot(range(len(Not_trained_mean)), Not_trained_mean, label=\"No Pre-training\", c=clrs[0])\n",
    "    ax.fill_between(range(len(Not_trained_mean)), Not_trained_mean-Not_trained_std, Not_trained_mean+Not_trained_std, alpha=0.3,facecolor=clrs[0])\n",
    "\n",
    "    ax.plot(range(len(Model2_Fidelity_mean)), Model2_Fidelity_mean, label=\"Model2 Fidelity Pre-training\", c=clrs[1])\n",
    "    ax.fill_between(range(len(Model2_Fidelity_mean)), Model2_Fidelity_mean-Model2_Fidelity_std, Model2_Fidelity_mean+Model2_Fidelity_std, alpha=0.3,facecolor=clrs[1])\n",
    "\n",
    "    #ax.plot(range(len(Model2_HSinner_mean)), Model2_HSinner_mean, label=\"Model2 HSinner Pre-training\", c=clrs[2])\n",
    "    #ax.fill_between(range(len(Model2_HSinner_mean)), Model2_HSinner_mean-Model2_HSinner_std, Model2_HSinner_mean+Model2_HSinner_std, alpha=0.3,facecolor=clrs[2])\n",
    "\n",
    "    ax.plot(range(2000), np.ones(2000) * LB_before_traindata, linestyle='dashed', linewidth=1.5, label=\"Lower Bound without Pre-training\", c=clrs[0])\n",
    "    ax.plot(range(2000), np.ones(2000) * LB_Fidelity_traindata, linestyle='dashed', linewidth=1.5, label=\"Lower Bound with Model2 Fidelity\", c=clrs[1])\n",
    "    #ax.plot(range(1000), np.ones(1000) * LB_HSinner_traindata, linestyle='dashed', linewidth=1.5, label=\"Lower Bound with Model2 HSinner\", c=clrs[2])\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"PCA+NQE+QCNN (8qubits) Loss History\")\n",
    "ax.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Loss Histories and Trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/jungguchoi/Library/Mobile Documents/com~apple~CloudDocs/1_Post_doc(Cleveland_clinic:2024.10~2025.09)/1_Research_project/3_quantum_embedding_comparison_sequence(2024.09 ~ XXXX.XX)/2_exp/9_NQE_QCNN_MNIST_8_qubits/Neural-Quantum-Embedding-main-8-qubits/Results/4_241021_8_qubits_cond2_PCA_NQE_QCNN(from_177_to_16_in_NN)/'\n",
    "f = open(save_dir+'/Loss_histories_and_weights.txt', 'w')\n",
    "\n",
    "for i in range(5):\n",
    "    f.write(f'Loss History Model2 Fidelity {i + 1}:')\n",
    "    f.write('\\n')\n",
    "    f.write(str(Loss_histories_Model2_Fidelity[i]))\n",
    "    f.write('\\n')\n",
    "for i in range(5):\n",
    "    f.write(f'Weights Model2 Fidelity {i + 1}:')\n",
    "    f.write('\\n')\n",
    "    f.write(str(weights_Model2_Fidelity[i]))\n",
    "\n",
    "#for i in range(5):\n",
    "#    f.write(f'Loss History Model2 HSinner {i + 1}:')\n",
    "#    f.write('\\n')\n",
    "#    f.write(str(Loss_histories_Model2_HSinner[i]))\n",
    "#    f.write('\\n')\n",
    "#for i in range(5):\n",
    "#    f.write(f'Weights Model2 HSinner {i + 1}:')\n",
    "#    f.write('\\n')\n",
    "#    f.write(str(weights_Model2_HSinner[i]))\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the accuracies of QCNN classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_test(predictions, labels):\n",
    "    acc = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if np.abs(l - p) < 1:\n",
    "            acc = acc + 1\n",
    "    return acc / len(labels)\n",
    "\n",
    "\n",
    "accuracies_not_trained = []\n",
    "accuracies_Model2_Fidelity, accuracies_Model2_HSinner = [], []\n",
    "\n",
    "for i in range(4):\n",
    "    prediction_not_trained = [QCNN_classifier(weights_not_trained[i], x, Trained=False) for x in X_test]\n",
    "    prediction_Model2_Fidelity = [QCNN_classifier(weights_Model2_Fidelity[i], x, Trained='Model2_Fidelity') for x in X_test]\n",
    "    #prediction_Model2_HSinner = [QCNN_classifier(weights_Model2_HSinner[i], x, Trained='Model2_HSinner') for x in X_test]\n",
    "    \n",
    "    accuracy_not_trained = accuracy_test(prediction_not_trained, Y_test)\n",
    "    accuracy_Model2_Fidelity = accuracy_test(prediction_Model2_Fidelity, Y_test)\n",
    "    #accuracy_Model2_HSinner = accuracy_test(prediction_Model2_HSinner, Y_test)\n",
    "\n",
    "    accuracies_not_trained.append(accuracy_not_trained)\n",
    "    accuracies_Model2_Fidelity.append(accuracy_Model2_Fidelity)\n",
    "    #accuracies_Model2_HSinner.append(accuracy_Model2_HSinner)\n",
    "\n",
    "accuracies_not_trained = np.array(accuracies_not_trained)\n",
    "accuracies_Model2_Fidelity = np.array(accuracies_Model2_Fidelity)\n",
    "#accuracies_Model2_HSinner = np.array(accuracies_Model2_HSinner)\n",
    "\n",
    "print(f\" Accuracy without pre-training: {accuracies_not_trained.mean()} ± {accuracies_not_trained.std()}\")\n",
    "print(f\" Accuracy after pre-training with Model2_Fidelity: {accuracies_Model2_Fidelity.mean()} ± {accuracies_Model2_Fidelity.std()}\")\n",
    "#print(f\" Accuracy after pre-training with Model2_HSinner: {accuracies_Model2_HSinner.mean()} ± {accuracies_Model2_HSinner.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f8e2a2ca6259a6cab5ca53f758a7b3aaf50f8fc283f46719ccb9cbe78a7dc03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
