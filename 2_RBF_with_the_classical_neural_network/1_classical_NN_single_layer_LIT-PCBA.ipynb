{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6234573f-5589-44ab-b795-780a964bf694",
   "metadata": {},
   "source": [
    "## Application of NN structure applied in the NQE method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d7c31c-b754-4c00-be53-004f9d2bf1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba2e0fd-3da3-4da1-bc01-63c3b5fdf6a5",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c9f142-8767-4cf7-9c04-23ce5058e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'ALDH1'\n",
    "sampling = '1_6'\n",
    "feature_reduction = False\n",
    "classes = [0,1]\n",
    "\n",
    "quantum_embed = 'ZZ'\n",
    "n_qubits = 8\n",
    "kernel = 'RBF'\n",
    "pretrained = True\n",
    "loss_function = 'BCE' # MSE / BCE / Linear\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 256\n",
    "learning_rate = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cb58b4-e35a-4e38-a12a-d7e0a3d23ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = data.data_load_and_process(dataset='protein', target=target, sampling=sampling, feature_reduction=feature_reduction, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f13af3-67dc-4254-94b7-3dbd579166b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train:\",X_train.shape,\"/ X_test:\",X_test.shape,\"/Y_train:\",Y_train.shape,\"/Y_test:\",Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd21f0ec-bbed-441f-b5e2-f8321d8da90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(Y_train), Counter(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ba4506-8d64-419c-a05b-21e94278103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float)\n",
    "\n",
    "num_pos = Y_train_tensor.sum()\n",
    "num_neg = len(Y_train_tensor) - num_pos\n",
    "\n",
    "pos_weight = num_neg / num_pos\n",
    "pos_weight = torch.tensor([pos_weight], dtype=torch.float)\n",
    "\n",
    "print(\"num_pos:\", num_pos.item(), \"/ num_neg:\", num_neg.item())\n",
    "print(\"pos_weight:\", pos_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f65623-4aeb-4a42-829c-ce028d9e9ddd",
   "metadata": {},
   "source": [
    "### Apply NN structure used in the NQE method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b004ac4-9531-4e9c-89bf-452e8f239d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_file_dir = '/Users/jungguchoi/Library/Mobile Documents/com~apple~CloudDocs/1_Post_doc(Cleveland_clinic:2024.10~2025.09)/1_Research_project/3_quantum_embedding_comparison_sequence(2024.09 ~ XXXX.XX)/2_exp/60_Dr_Park_Meeting_and_comments_SEP1725/2_new_classical_counterparts/15_ALDH1_NN_RBF_1_6_ratio/MLP1_LIT-PCBA_ALDH1_1_6_sampling_MLP_ZZ_8_qubits(RBF).pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136df77a-a90e-4b58-b1a0-568bb1ffcfaf",
   "metadata": {},
   "source": [
    "1. MLP structure without the pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b85ba7-9c1d-42e8-8131-97f6258a0454",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(39, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.layer4 = nn.Linear(32, 16)\n",
    "        self.layer5 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.relu(self.layer4(x))\n",
    "        x = self.sigmoid(self.layer5(x))\n",
    "        return x\n",
    "\n",
    "model_NN = NN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96210ba5-b79e-4979-a9f5-8fe72cf8c553",
   "metadata": {},
   "source": [
    "2. MLP structure with the pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9575dd7-7730-4fc5-9a58-05c6fdff011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = torch.load(parameter_file_dir, map_location=\"cpu\", weights_only=True)\n",
    "\n",
    "pretrained_MLP_body = nn.Sequential(\n",
    "    nn.Linear(39, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16)\n",
    ")\n",
    "\n",
    "pretrained_MLP_body.load_state_dict({\n",
    "    '0.weight': pretrained['linear_relu_stack.0.weight'],\n",
    "    '0.bias': pretrained['linear_relu_stack.0.bias'],\n",
    "    '2.weight': pretrained['linear_relu_stack.2.weight'],\n",
    "    '2.bias': pretrained['linear_relu_stack.2.bias'],\n",
    "    '4.weight': pretrained['linear_relu_stack.4.weight'],\n",
    "    '4.bias': pretrained['linear_relu_stack.4.bias'],\n",
    "    '6.weight': pretrained['linear_relu_stack.6.weight'],\n",
    "    '6.bias': pretrained['linear_relu_stack.6.bias'],\n",
    "    '8.weight': pretrained['linear_relu_stack.8.weight'],\n",
    "    '8.bias': pretrained['linear_relu_stack.8.bias'],\n",
    "    '10.weight': pretrained['linear_relu_stack.10.weight'],\n",
    "    '10.bias': pretrained['linear_relu_stack.10.bias'],\n",
    "    '12.weight': pretrained['linear_relu_stack.12.weight'],\n",
    "    '12.bias': pretrained['linear_relu_stack.12.bias'],\n",
    "    # '14.weight': pretrained['linear_relu_stack.14.weight'],\n",
    "    # '14.bias': pretrained['linear_relu_stack.14.bias'],\n",
    "    # '16.weight': pretrained['linear_relu_stack.16.weight'],\n",
    "    # '16.bias': pretrained['linear_relu_stack.16.bias']\n",
    "})\n",
    "\n",
    "pretrained_MLP_body.eval()\n",
    "\n",
    "class ExtendedNN(nn.Module):\n",
    "    def __init__(self, pretrained_body):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = pretrained_MLP_body\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16, 1),\n",
    "            #nn.Tanh()\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "pretrained_model_NN = ExtendedNN(pretrained_MLP_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4b0d11-f8c3-47c1-b1f0-75f1a5df1ee1",
   "metadata": {},
   "source": [
    "3. Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4bf7d-1dd5-4200-9787-9cb769a9c3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_Loss(predictions, labels):\n",
    "    loss = 0\n",
    "    labels = 2 * labels - 1\n",
    "    for l,p in zip(labels, predictions):\n",
    "        loss += 0.5 * (1 - l * p)\n",
    "    return loss / len(labels)\n",
    "\n",
    "loss_fn_BCE = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "loss_fn_MSE = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6925475-8594-4fd0-8a81-cbe55a6a9b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, X_train, y_train, X_val, y_val):\n",
    "    loss_history=[]\n",
    "    \n",
    "    if loss_function == 'MSE':\n",
    "        loss_fn = loss_fn_MSE\n",
    "    elif loss_function == 'BCE':\n",
    "        loss_fn = loss_fn_BCE\n",
    "    elif loss_function == 'Linear':\n",
    "        loss_fn = Linear_Loss\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        loss_value=0\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                #print(\"y_pred:\",y_pred.shape,\"/y_batch:\", y_batch.shape)\n",
    "                loss = loss_fn(y_pred.view(-1), y_batch)\n",
    "                loss_value+=loss.detach().numpy()\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "            \n",
    "            \n",
    "            loss_history.extend([loss_value/batch_size])\n",
    "            if epoch % 200 == 0:\n",
    "                print(\"Epoch:\",epoch,\"/ Loss:\",loss_value/batch_size)\n",
    "            \n",
    "    # evaluate accuracy at end of each epoch\n",
    "    model.eval()\n",
    "    y_pred = model(X_val)\n",
    "    if loss_function == 'Linear':\n",
    "        predicted_values = torch.where(y_pred < 0, -1, 1)\n",
    "        print(\"Prediction:\", Counter(predicted_values.squeeze().tolist()))\n",
    "        y_val[y_val == 0] = -1\n",
    "        acc = accuracy_score(y_val, predicted_values)\n",
    "    else:\n",
    "        probs = torch.sigmoid(y_pred)\n",
    "        predicted_values = (probs >= 0.5).int().numpy()\n",
    "        print(\"Prediction:\", Counter(predicted_values.flatten()))\n",
    "        acc = balanced_accuracy_score(y_val, predicted_values)\n",
    "    \n",
    "    return acc, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a489b-cab1-448e-b138-e0655ed47ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_full=[]\n",
    "acc_full=[]\n",
    "for no in range(5):\n",
    "    acc, loss_history = model_train(model=pretrained_model_NN,\n",
    "                                    X_train=torch.Tensor(X_train),\n",
    "                                    y_train=torch.Tensor(Y_train),\n",
    "                                    X_val=torch.Tensor(X_test),\n",
    "                                    y_val=torch.Tensor(Y_test))\n",
    "    \n",
    "    print(\"No:\",str(no),\"/ Accuracy:\", acc)\n",
    "    print(\"---------------------------------\")\n",
    "    loss_full.append(loss_history)\n",
    "    acc_full.extend([acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7692dc-731c-4faf-9d92-92c87993249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_full_mean = np.mean(np.array(acc_full))\n",
    "acc_full_std = np.std(np.array(acc_full))\n",
    "print(acc_full_mean, acc_full_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5bf758-22ce-4e9b-80c4-ca5fa3a2656e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(loss_full))\n",
    "loss_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aed8f1-6b6a-4758-92c2-f92996a73c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_full_mean = np.array(loss_full).mean(axis=0)\n",
    "loss_full_std = np.array(loss_full).std(axis=0)\n",
    "print(len(loss_full_mean))\n",
    "print(len(loss_full_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08adc9e5-9832-47b9-b3d5-54eecf675f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/jungguchoi/Library/Mobile Documents/com~apple~CloudDocs/1_Post_doc(Cleveland_clinic:2024.10~2025.09)/1_Research_project/3_quantum_embedding_comparison_sequence(2024.09 ~ XXXX.XX)/2_exp/60_Dr_Park_Meeting_and_comments_SEP1725/2_new_classical_counterparts/15_ALDH1_NN_RBF_1_6_ratio/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913bf980-4d6b-4de7-b8f7-9bf059e82de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(save_dir+'Loss_histories_and_weights('+str(n_epochs)+'iter_'+str(batch_size)+'batch_'+str(learning_rate)+'lr_'+str(loss_function)+').txt', 'w')\n",
    "\n",
    "for i in range(5):\n",
    "    f.write(f'Loss History {i + 1}:')\n",
    "    f.write('\\n')\n",
    "    f.write(str(loss_full[i]))\n",
    "    f.write('\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ae15a-c240-4dc8-a9fc-1f04b380876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 5] \n",
    "fig, ax = plt.subplots() \n",
    "clrs = sns.color_palette(\"husl\", 3) \n",
    "with sns.axes_style(\"darkgrid\"): \n",
    "    ax.plot(range(len(loss_full_mean)), loss_full_mean, label=\"MLP+single layer model\", c=clrs[1]) \n",
    "    ax.fill_between(range(len(loss_full_mean)), loss_full_mean-loss_full_std, loss_full_mean+loss_full_std, alpha=0.3,facecolor=clrs[1])\n",
    "\n",
    "ax.set_xlabel(\"Iteration\") \n",
    "ax.set_ylabel(\"Loss\") \n",
    "ax.set_title(\"MLP + single layer Loss History\") \n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
